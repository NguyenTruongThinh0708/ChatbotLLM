import sys
import os
import torch
import streamlit as st
import logging
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from embedder import EmbeddingGenerator
from vector_db import VectorDB
from retriever import Retriever
from generator import Generator

logger = logging.getLogger(__name__)

@st.cache_resource
def init_components():
    logger.info("[LOG] Khá»Ÿi táº¡o cÃ¡c module...")

    logger.info("[LOG] -> Khá»Ÿi táº¡o EmbeddingGenerator...")
    embedder = EmbeddingGenerator()
    logger.info("[LOG] âœ… EmbeddingGenerator OK")

    logger.info("[LOG] -> Khá»Ÿi táº¡o VectorDB...")
    vector_db = VectorDB()
    logger.info("[LOG] âœ… VectorDB OK")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"[LOG] -> Khá»Ÿi táº¡o Retriever (device={device})...")
    retriever = Retriever(vector_db, device=device)
    logger.info("[LOG] âœ… Retriever OK")

    logger.info("[LOG] -> Khá»Ÿi táº¡o Generator...")
    generator = Generator(embedder, retriever)
    logger.info("[LOG] âœ… Generator OK")

    logger.info("[LOG] ğŸ‰ Táº¥t cáº£ module khá»Ÿi táº¡o thÃ nh cÃ´ng.")

    return embedder, vector_db, retriever, generator

embedder, vector_db, retriever, generator = init_components()

# ==========================
# HÃ m build_conversation
# ==========================
def build_conversation(messages, last_n=2):
    """
    Chuyá»ƒn lá»‹ch sá»­ há»™i thoáº¡i thÃ nh string cho LLM.
    Giá»¯ láº¡i tá»‘i Ä‘a last_n lÆ°á»£t (user + assistant).
    """
    history = []
    for msg in messages[-last_n*2:]:
        role = "NgÆ°á»i dÃ¹ng" if msg["role"] == "user" else "Trá»£ lÃ½"
        history.append(f"{role}: {msg['content']}")
    return "\n".join(history)

# UI
st.title("Medical RAG Chatbot")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if query := st.chat_input("Nháº­p cÃ¢u há»i: "):
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    st.write("[LOG] Äang xá»­ lÃ½ cÃ¢u há»i...")

    # Láº¥y láº¡i tá»‘i Ä‘a 2 lÆ°á»£t gáº§n nháº¥t
    conversation_history = build_conversation(st.session_state.messages, last_n=2)

    processed_query = embedder.preprocess_and_tokenize(query)
    query_embedding = embedder.embed_query(processed_query)
    retrieve_results = retriever.retrieve(query_embedding)

    if not retrieve_results:
        response = "âŒ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u liÃªn quan."
    else:
        documents = [r.payload["content"] for r in retrieve_results]
        context = "\n".join([f"{i+1}. {content}" for i, content in enumerate(documents)])

        # Truyá»n thÃªm history vÃ o generator
        response = generator.generate(query, context, history=conversation_history)

    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.markdown(response)
